---
sidebar_position: 07.02
---

# Quantitative risk assessment
========================

![img](/img/1-7-2-1.png)

![img](/img/1-7-2-2.png)

When we're able to gather quantitative data about our assets and risks, we can use that information to make data informed decisions about risk. 

The process of using numeric data to assist in risk decisions is known as quantitative risk management. Security professionals performing a quantitative risk assessment do so for a single risk asset pairing at a time. For example, they might conduct an assessment based upon the risk of flooding to a data center. As they conduct the assessment, they must first determine the values for several variables. The first of these is the asset value, or Av. This is quite simply the estimated value in dollars of that asset. Risk assessors determining an assets value have several options available to them. The original cost technique simply looks at the invoices from an asset purchase, and uses those purchase prices to determine the asset value. This is the easiest technique to perform because it simply requires looking at those invoices. However, it's often criticized because the cost to actually replace an asset may be significantly higher or lower if asset prices have changed since the purchase. The depreciated cost technique is an accounting favorite. This approach begins with the original cost of the asset and then reduces its value over time as the asset ages. The depreciation technique uses an estimate of the asset's useful life and then gradually decreases the asset value until it reaches zero at the end of its projected lifespan. The replacement cost technique is the most popular among risk managers because it produces results that most closely approximate the actual costs that an organization will incur. It goes out and looks at current supplier prices to determine the cost of replacing an asset in the current market, and then uses that cost as the asset's value. We might use this technique to value our data center at $20 million, because that's the amount of money that would be required to rebuild it after a disaster. The second variable that we must consider is the exposure factor or EF. The exposure factor is based upon the specific risks considered in the risk assessment, and it estimates the percentage of an asset that will be damaged if that risk materializes. For example, if we expect a flood might damage 50% of our data center, we'd set the exposure factor for flood to 50%. The next variable is the single loss expectancy, or SLE. This is the actual damage that we expect to occur if a risk materializes one time. We compute the SLE by multiplying the asset value by the exposure factor. So if we have a data center valued at $20 million, and we expect that a flood would cause 50% damage to the facility, we compute our SLE by multiplying these two numbers together, finding that a single flood would cause $10 million in damage. That's the impact of the risk. Now the SLE only gives us an idea of impact. And as you know, risk assessment must also consider the likelihood of a risk. That's where the annualized rate of occurrence or ARO comes into play. The ARO is the number of times each year that we expect a risk to occur. Now in the case of a flood, we might consult FEMA Flood Maps to determine that there's a 1% annual risk of flood in the vicinity of our data center. That's the same as saying that we expect 0.01 floods to occur each year so our ARO is 0.01. Now our risk analysis needs to incorporate both of these likelihood and impact values. We do this by computing the annualized loss expectancy or ALE. The ALE is the amount of money we expect to lose each year from that risk asset pairing, and it's a good measure of overall risk. We compute the ALE by multiplying the single loss expectancy and the annualized rate of occurrence together. In the case of that flood risk to our data center, the SLE was $10 million, and the ARO was 0.01. Multiplying these together, we get an annualized loss expectancy of $100,000. We expect to lose $100,000 each year from the risk of flooding to our data center. Now it's important to remember that cost won't occur every year. In reality, we'll have $10 million of damage each time a flood occurs but we only expect that to happen once every 100 years so that averages out to $100,000 per year. And that's how you perform a quantitative risk analysis. You should definitely memorize these formulas and be prepared to compute them on the exam. Quantitative techniques also help us assess our ability to restore IT services and components quickly in the event of a failure. We do this by looking at several time values. The values we use depend upon whether an asset is repairable or non repairable. That is whether we can fix it or whether it needs to be replaced. For non repairable assets, those that we cannot fix, our most important metric is the mean time to failure. That's the amount of time that we expect will pass before an asset fails. When using mean values, it's important to remember the meaning of average. Half of the assets of that type will fail before the MTTF, and half will last longer than the MTTF. Mean values are useful for planning purposes, but they shouldn't be completely depended upon. If our asset is repairable, we look at two different values. The first is the mean time between failures or MTBF. Now that's quite similar to the MTTF. It's simply the average amount of time that passes between failures of a repairable asset. The second value that we track for repairable assets is the mean time to repair or MTTR. This is the amount of time that an asset will be out of service for repair each time that it fails. When we look at the MTTR and MTBF values together, we can get a good idea of the expected downtime for an IT component.

![img](/img/1-7-2-3.png)

![img](/img/1-7-2-4.png)

![img](/img/1-7-2-5.png)

![img](/img/1-7-2-6.png)
